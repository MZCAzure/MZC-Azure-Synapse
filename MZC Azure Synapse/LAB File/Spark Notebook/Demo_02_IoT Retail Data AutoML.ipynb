{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CosmosDB Configuration"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load the IoTDeviceInfo dataset from ADLS Gen2 to a dataframe\r\n",
        ">The Synapse workspace is attached to an ADLS Gen2 storage account and the files placed on the default storage account can be accessed using the relative path as below.\r\n",
        "&nbsp;"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfDeviceInfo = (spark\r\n",
        "                .read\r\n",
        "                .csv(\"abfss://cosmosdemo@<ADLS Gen2 Account Name>.dfs.core.windows.net/SynapseDemoIoT/IoTDeviceInfo.csv\", header=True)\r\n",
        "              )\r\n",
        "\r\n",
        "dfSignals = (spark\r\n",
        "                .read\r\n",
        "                .csv(\"abfss://cosmosdemo@<ADLS Gen2 Account Name>.dfs.core.windows.net/SynapseDemoIoT/IoTSignals.csv\", header=True)\r\n",
        "              )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfProducts = (spark\r\n",
        "                .read\r\n",
        "                .csv(\"abfss://cosmosdemo@<ADLS Gen2 Account Name>.dfs.core.windows.net/SynapseDemoRetail/Products.csv\", header=True)\r\n",
        "              )\r\n",
        "\r\n",
        "dfRetailSales = (spark\r\n",
        "                .read\r\n",
        "                .csv(\"abfss://cosmosdemo@<ADLS Gen2 Account Name>.dfs.core.windows.net/SynapseDemoRetail/RetailSales.csv\", header=True)\r\n",
        "              )\r\n",
        "\r\n",
        "dfStoreDemographics = (spark\r\n",
        "                .read\r\n",
        "                .csv(\"abfss://cosmosdemo@<ADLS Gen2 Account Name>.dfs.core.windows.net/SynapseDemoRetail/StoreDemoGraphics.csv\", header=True)\r\n",
        "              )\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Write the dataframe to the Azure Cosmos DB collection\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfDeviceInfo.write\\\r\n",
        "            .format(\"cosmos.oltp\")\\\r\n",
        "            .option(\"spark.synapse.linkedService\", \"CosmosDemo\")\\\r\n",
        "            .option(\"spark.cosmos.container\", \"IoTDeviceInfo\")\\\r\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
        "            .mode('append')\\\r\n",
        "            .save()\r\n",
        "\r\n",
        "dfSignals.write\\\r\n",
        "            .format(\"cosmos.oltp\")\\\r\n",
        "            .option(\"spark.synapse.linkedService\", \"CosmosDemo\")\\\r\n",
        "            .option(\"spark.cosmos.container\", \"IoTSignals\")\\\r\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
        "            .mode('append')\\\r\n",
        "            .save()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfProducts.write\\\r\n",
        "            .format(\"cosmos.oltp\")\\\r\n",
        "            .option(\"spark.synapse.linkedService\", \"CosmosDemo\")\\\r\n",
        "            .option(\"spark.cosmos.container\", \"RetailProducts\")\\\r\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
        "            .mode('append')\\\r\n",
        "            .save()\r\n",
        "\r\n",
        "dfRetailSales.write\\\r\n",
        "            .format(\"cosmos.oltp\")\\\r\n",
        "            .option(\"spark.synapse.linkedService\", \"CosmosDemo\")\\\r\n",
        "            .option(\"spark.cosmos.container\", \"RetailSales\")\\\r\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
        "            .mode('append')\\\r\n",
        "            .save()\r\n",
        "\r\n",
        "dfStoreDemographics.write\\\r\n",
        "            .format(\"cosmos.oltp\")\\\r\n",
        "            .option(\"spark.synapse.linkedService\", \"CosmosDemo\")\\\r\n",
        "            .option(\"spark.cosmos.container\", \"RetailStoreDemographics\")\\\r\n",
        "            .option(\"spark.cosmos.write.upsertEnabled\", \"true\")\\\r\n",
        "            .mode('append')\\\r\n",
        "            .save()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Simulate streaming data generation using Rate streaming source\r\n",
        "* The Rate streaming source is used to simplify the solution here and can be replaced with any supported streaming sources such as [Azure Event Hubs](https://azure.microsoft.com/en-us/services/event-hubs/) and [Apache Kafka](https://docs.microsoft.com/en-us/azure/hdinsight/kafka/apache-kafka-introduction).\r\n",
        "\r\n",
        "\r\n",
        ">The Rate streaming source generates data at the specified number of rows per second and each output row contains a timestamp and value.\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfStream = (spark\r\n",
        "                .readStream\r\n",
        "                .format(\"rate\")\r\n",
        "                .option(\"rowsPerSecond\", 10)\r\n",
        "                .load()\r\n",
        "            )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Format the stream dataframe as per the IoTSignals schema\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.types import StringType\r\n",
        "import uuid\r\n",
        "\r\n",
        "numberOfDevices = 10\r\n",
        "generate_uuid = F.udf(lambda : str(uuid.uuid4()), StringType())\r\n",
        "              \r\n",
        "dfIoTSignals = (dfStream\r\n",
        "                    .withColumn(\"id\", generate_uuid())\r\n",
        "                    .withColumn(\"dateTime\", dfStream[\"timestamp\"].cast(StringType()))\r\n",
        "                    .withColumn(\"deviceId\", F.concat(F.lit(\"dev-\"), F.expr(\"mod(value, %d)\" % numberOfDevices)+1))\r\n",
        "                    .withColumn(\"measureType\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Rotation Speed' ELSE 'Output' END\"))\r\n",
        "                    .withColumn(\"unitSymbol\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'RPM' ELSE 'MW' END\"))\r\n",
        "                    .withColumn(\"unit\", F.expr(\"CASE WHEN rand() < 0.5 THEN 'Revolutions per Minute' ELSE 'MegaWatts' END\"))\r\n",
        "                    .withColumn(\"measureValue\", F.expr(\"CASE WHEN rand() > 0.9 THEN value * 2 WHEN rand() < 0.1 THEN value div 2 ELSE value END\"))\r\n",
        "                    .drop(\"timestamp\")\r\n",
        "                )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Stream writes to the Azure Cosmos DB Collection\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\r\n",
        "\r\n",
        "streamQuery = dfIoTSignals\\\r\n",
        "        .writeStream\\\r\n",
        "        .format(\"cosmos.oltp\")\\\r\n",
        "        .outputMode(\"append\")\\\r\n",
        "        .option(\"checkpointLocation\", \"/writeCheckpointDir\")\\\r\n",
        "        .option(\"spark.synapse.linkedService\", \"CosmosDemo\")\\\r\n",
        "        .option(\"spark.cosmos.container\", \"IoTStreamingSignals\")\\\r\n",
        "        .option(\"spark.cosmos.connection.mode\", \"gateway\")\\\r\n",
        "        .start()\r\n",
        "\r\n",
        "time.sleep(120)\r\n",
        "streamQuery.stop()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IoT Investigation Scenario"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create Spark tables pointing to the Azure Cosmos DB Analytical Store collections using Azure Synapse Link \r\n",
        "\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "create database CosmosDemoIoT"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "create database CosmosDemoRetail"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "create table if not exists CosmosDemoIoT.IoTSignals\r\n",
        "using cosmos.olap\r\n",
        "options(spark.synapse.linkedService 'CosmosDemo',\r\n",
        "        spark.cosmos.container 'IoTSignals')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "create table if not exists CosmosDemoIoT.IoTDeviceInfo\r\n",
        "using cosmos.olap\r\n",
        "options(spark.synapse.linkedService 'CosmosDemo',\r\n",
        "        spark.cosmos.container 'IoTDeviceInfo')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "\n",
        "create table if not exists CosmosDemoRetail.RetailProducts\n",
        "using cosmos.olap\n",
        "options(spark.synapse.linkedService 'CosmosDemo',\n",
        "        spark.cosmos.container 'RetailProducts')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "\n",
        "create table if not exists CosmosDemoRetail.RetailSales\n",
        "using cosmos.olap\n",
        "options(spark.synapse.linkedService 'CosmosDemo',\n",
        "        spark.cosmos.container 'RetailSales')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "\n",
        "create table if not exists CosmosDemoRetail.RetailStoreDemographics\n",
        "using cosmos.olap\n",
        "options(spark.synapse.linkedService 'CosmosDemo',\n",
        "        spark.cosmos.container 'RetailStoreDemographics')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Perform Joins across collections, apply filters and aggregations using Spark SQL \r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_RPM_details = spark.sql(\"select a.deviceid \\\r\n",
        "                                 , b.devicetype \\\r\n",
        "                                 , cast(b.location as string) as location\\\r\n",
        "                                 , cast(b.latitude as float) as latitude\\\r\n",
        "                                 , cast(b.longitude as float) as  longitude\\\r\n",
        "                                 , a.measuretype \\\r\n",
        "                                 , a.unitSymbol \\\r\n",
        "                                 , cast(sum(measureValue) as float) as measureValueSum \\\r\n",
        "                                 , count(*) as count \\\r\n",
        "                            from CosmosDemoIoT.IoTSignals a \\\r\n",
        "                            left join CosmosDemoIoT.IoTDeviceInfo b \\\r\n",
        "                            on a.deviceid = b.deviceid \\\r\n",
        "                            where a.unitSymbol = 'RPM' \\\r\n",
        "                            group by a.deviceid, b.devicetype, b.location, b.latitude, b.longitude, a.measuretype, a.unitSymbol\")\r\n",
        "\r\n",
        "display(df_RPM_details)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Visualizations using plotly and displayHTML()\r\n",
        "The below shows a heatmap of IoT signals across diffrent locations\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.offline import plot\r\n",
        "import plotly.express as px\r\n",
        "\r\n",
        "df_RPM_details_pd = df_RPM_details.toPandas()\r\n",
        "fig = px.scatter_mapbox(df_RPM_details_pd, \r\n",
        "                        lat='latitude', \r\n",
        "                        lon='longitude', \r\n",
        "                        size = 'measureValueSum',\r\n",
        "                        color = 'measureValueSum',\r\n",
        "                        hover_name = 'location',\r\n",
        "                        hover_data = ['measureValueSum','location'],\r\n",
        "                        size_max = 30,\r\n",
        "                        color_continuous_scale = px.colors.carto.Temps,\r\n",
        "                        zoom=3,\r\n",
        "                        height=600,\r\n",
        "                        width =900)\r\n",
        "\r\n",
        "fig.update_layout(mapbox_style='open-street-map')\r\n",
        "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\r\n",
        "\r\n",
        "p = plot(fig,output_type='div')\r\n",
        "displayHTML(p)       "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Loda the data in Cosmos DB Analytical store collection "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_IoTSignals = spark.read\\\r\n",
        "                    .format(\"cosmos.olap\")\\\r\n",
        "                    .option(\"spark.synapse.linkedService\", \"CosmosDemo\")\\\r\n",
        "                    .option(\"spark.cosmos.container\", \"IoTSignals\")\\\r\n",
        "                    .load()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data exploration using pyplot"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "df_IoTSignals_pd = df_IoTSignals.toPandas()\r\n",
        "df_dev = df_IoTSignals_pd[(df_IoTSignals_pd.deviceId == \"dev-1\")]\r\n",
        "df_dev = df_dev.dropna()\r\n",
        "df_dev = df_dev.astype({\"measureValue\": int})\r\n",
        "#display(df_dev)\r\n",
        "df_dev = df_dev.pivot(index='dateTime', columns = 'unitSymbol' , values =  'measureValue')\r\n",
        "df_dev['timestamp']=df_dev.index\r\n",
        "df_dev['index']=list(range(len(df_dev)))\r\n",
        "df_dev.set_index('index',inplace=True)\r\n",
        "df_dev.plot(y='MW', x= 'timestamp', color='green',figsize=(20,5), label = 'Output MW')\r\n",
        "plt.title('MW TimeSeries')\r\n",
        "df_dev.plot(y='RPM', x= 'timestamp', color='black', figsize=(20,5), label = 'RPM')\r\n",
        "plt.title('RPM TimeSeries')\r\n",
        "plt.legend(loc = 'best')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Perform anomaly detection using Microsoft Machine Learning for Spark (MMLSpark)"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\r\n",
        "from mmlspark.cognitive import SimpleDetectAnomalies\r\n",
        "from mmlspark.core.spark import FluentAPI\r\n",
        "\r\n",
        "anomaly_detector = (SimpleDetectAnomalies()\r\n",
        "                            .setSubscriptionKey(\"<Anomaly Detector Access Key>\")\r\n",
        "                            .setUrl(\"<Anomaly Detector Endpoint>/anomalydetector/v1.0/timeseries/entire/detect\")\r\n",
        "                            .setLocation('koreacentral')\r\n",
        "                            .setOutputCol(\"anomalies\")\r\n",
        "                            .setGroupbyCol(\"grouping\")\r\n",
        "                            .setSensitivity(95)\r\n",
        "                            .setGranularity(\"secondly\"))\r\n",
        "\r\n",
        "df_anomaly = (df_IoTSignals\r\n",
        "                    .where(col(\"unitSymbol\") == 'RPM')\r\n",
        "                    .withColumnRenamed(\"dateTime\", \"timestamp\")\r\n",
        "                    .withColumn(\"value\", col(\"measureValue\").cast(\"double\"))\r\n",
        "                    .withColumn(\"grouping\", col(\"deviceId\"))\r\n",
        "                    .mlTransform(anomaly_detector))\r\n",
        "\r\n",
        "df_anomaly.createOrReplaceTempView('df_anomaly')\r\n",
        "\r\n",
        "display(df_anomaly)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Format the dataframe for visualization"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_anomaly_single_device = spark.sql(\"select timestamp \\\r\n",
        "                                            , measureValue \\\r\n",
        "                                            , anomalies.expectedValue \\\r\n",
        "                                            , anomalies.expectedValue + anomalies.upperMargin as expectedUpperValue \\\r\n",
        "                                            , anomalies.expectedValue - anomalies.lowerMargin as expectedLowerValue \\\r\n",
        "                                            , case when anomalies.isAnomaly=true then 1 else 0 end as isAnomaly \\\r\n",
        "                                        from df_anomaly \\\r\n",
        "                                        where deviceid = 'dev-1' and timestamp < '2020-12-29'\\\r\n",
        "                                        order by timestamp \\\r\n",
        "                                        limit 200\")\r\n",
        "\r\n",
        "display(df_anomaly_single_device)  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Visualize the anomalies using plotly\r\n",
        "* Plot Expected value, Upper Value, Lower Value and Actual Value along with Anomaly flag"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import chart_studio.plotly as py\r\n",
        "import plotly.graph_objs as go\r\n",
        "from plotly.offline import plot\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from pyspark.sql.functions import col\r\n",
        "from matplotlib.pyplot import figure\r\n",
        " \r\n",
        "adf = df_anomaly_single_device.toPandas()\r\n",
        "adf_subset = df_anomaly_single_device.where(col(\"isAnomaly\") == 1).toPandas() \r\n",
        "\r\n",
        "plt.figure(figsize=(23,8))\r\n",
        "plt.plot(adf['timestamp'],adf['expectedUpperValue'], color='darkred', linestyle='solid', linewidth=0.25)\r\n",
        "plt.plot(adf['timestamp'],adf['expectedValue'], color='darkgreen', linestyle='solid', linewidth=2)\r\n",
        "plt.plot(adf['timestamp'],adf['measureValue'], 'b', color='royalblue', linestyle='dotted', linewidth=2)\r\n",
        "plt.plot(adf['timestamp'],adf['expectedLowerValue'],  color='black', linestyle='solid', linewidth=0.25)\r\n",
        "plt.plot(adf_subset['timestamp'],adf_subset['measureValue'], 'ro')\r\n",
        "plt.legend(['RPM-UpperMargin', 'RPM-ExpectedValue', 'RPM-ActualValue', 'RPM-LowerMargin', 'RPM-Anomaly'])\r\n",
        "plt.title('RPM Anomalies with Expected, Actual, Upper and Lower Values')\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Surface Notebook Sales Prediction Model via Auto ML"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 9. Perform Joins across collections, apply filters and aggregations using Spark SQL from Azure Cosmos DB\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data = spark.sql(\"select a.storeId \\\n",
        "                       , b.productCode \\\n",
        "                       , b.wholeSaleCost \\\n",
        "                       , b.basePrice \\\n",
        "                       , c.ratioAge60 \\\n",
        "                       , c.collegeRatio \\\n",
        "                       , c.income \\\n",
        "                       , c.highIncome150Ratio \\\n",
        "                       , c.largeHH \\\n",
        "                       , c.minoritiesRatio \\\n",
        "                       , c.more1FullTimeEmployeeRatio \\\n",
        "                       , c.distanceNearestWarehouse \\\n",
        "                       , c.salesNearestWarehousesRatio \\\n",
        "                       , c.avgDistanceNearest5Supermarkets \\\n",
        "                       , c.salesNearest5StoresRatio \\\n",
        "                       , a.quantity \\\n",
        "                       , a.logQuantity \\\n",
        "                       , a.advertising \\\n",
        "                       , a.price \\\n",
        "                       , a.weekStarting \\\n",
        "                 from CosmosDemoRetail.RetailSales a \\\n",
        "                 left join CosmosDemoRetail.RetailProducts b \\\n",
        "                 on a.productcode = b.productcode \\\n",
        "                 left join CosmosDemoRetail.RetailStoreDemographics c \\\n",
        "                 on a.storeId = c.storeId \\\n",
        "                 order by a.weekStarting, a.storeId, b.productCode\")\n",
        "\n",
        "display(data)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "storeId"
            ],
            "values": [
              "storeId"
            ],
            "yLabel": "storeId",
            "xLabel": "storeId",
            "aggregation": "COUNT",
            "aggByBackend": false
          },
          "aggData": "{\"storeId\":{\"2\":12,\"5\":12,\"8\":12,\"9\":12,\"12\":12,\"14\":12,\"18\":12,\"21\":12,\"28\":12,\"32\":12,\"33\":12,\"40\":12,\"44\":9,\"45\":12,\"47\":12,\"48\":12,\"49\":12,\"50\":12,\"51\":12,\"52\":12,\"53\":12,\"54\":12,\"56\":12,\"59\":12,\"62\":12,\"64\":12,\"67\":12,\"68\":9,\"70\":12,\"71\":12,\"72\":12,\"73\":12,\"74\":12,\"75\":12,\"76\":12,\"77\":12,\"78\":12,\"80\":12,\"81\":12,\"83\":12,\"84\":12,\"86\":12,\"88\":12,\"89\":12,\"90\":12,\"91\":12,\"92\":12,\"93\":9,\"94\":12,\"95\":12,\"97\":12,\"98\":12,\"100\":15,\"101\":15,\"102\":15,\"103\":15,\"104\":12,\"105\":15,\"106\":15,\"107\":14,\"109\":12,\"110\":12,\"111\":12,\"112\":12,\"113\":12,\"114\":12,\"115\":12,\"116\":12,\"117\":12,\"118\":12,\"119\":12,\"121\":12,\"122\":12,\"123\":12,\"124\":12,\"126\":12,\"128\":12,\"129\":9,\"130\":12,\"131\":12,\"132\":12,\"134\":9,\"137\":12}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Azure Machine Learning's environment setup for AutoML to build a Forecasting Model\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import azureml.core\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from azureml.core.workspace import Workspace\n",
        "from azureml.core import Workspace\n",
        "from azureml.core.experiment import Experiment\n",
        "from azureml.train.automl import AutoMLConfig\n",
        "import os\n",
        "subscription_id = os.getenv(\"SUBSCRIPTION_ID\", default=\"<Your Subscription ID>\")\n",
        "resource_group = os.getenv(\"RESOURCE_GROUP\", default=\"mzc-rg\")\n",
        "workspace_name = os.getenv(\"WORKSPACE_NAME\", default=\"mzcmlworkspace\")\n",
        "workspace_region = os.getenv(\"WORKSPACE_REGION\", default=\"Korea Central\")\n",
        "\n",
        "ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
        "ws.write_config()\n",
        "    \n",
        "experiment_name = 'automl-surfaceforecasting'\n",
        "experiment = Experiment(ws, experiment_name)\n",
        "output = {}\n",
        "output['Subscription ID'] = ws.subscription_id\n",
        "output['Workspace'] = ws.name\n",
        "output['SKU'] = ws.sku\n",
        "output['Resource Group'] = ws.resource_group\n",
        "output['Location'] = ws.location\n",
        "output['Run History Name'] = experiment_name\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "outputDf = pd.DataFrame(data = output, index = [''])\n",
        "outputDf.T"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Data Preparation - Feature engineering, Splitting train & test datasets\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial variables\n",
        "time_column_name = 'weekStarting'\n",
        "grain_column_names = ['storeId', 'productCode']\n",
        "target_column_name = 'quantity'\n",
        "use_stores = [2, 5, 8,71,102]\n",
        "n_test_periods = 20\n",
        "\n",
        "\n",
        "#DataFrame\n",
        "df = data.toPandas()\n",
        "df[time_column_name] = pd.to_datetime(df[time_column_name])\n",
        "df['storeId'] = pd.to_numeric(df['storeId'])\n",
        "df['quantity'] = pd.to_numeric(df['quantity'])\n",
        "df['advertising'] = pd.to_numeric(df['advertising'])\n",
        "df['price'] = df['price'].astype(float)\n",
        "df['basePrice'] = df['basePrice'].astype(float)\n",
        "df['ratioAge60'] = df['ratioAge60'].astype(float)\n",
        "df['collegeRatio'] = df['collegeRatio'].astype(float)\n",
        "df['highIncome150Ratio'] = df['highIncome150Ratio'].astype(float)\n",
        "df['income'] = df['income'].astype(float)\n",
        "df['largeHH'] = df['largeHH'].astype(float)\n",
        "df['minoritiesRatio'] = df['minoritiesRatio'].astype(float)\n",
        "df['logQuantity'] = df['logQuantity'].astype(float)\n",
        "df['more1FullTimeEmployeeRatio'] = df['more1FullTimeEmployeeRatio'].astype(float)\n",
        "df['distanceNearestWarehouse'] = df['distanceNearestWarehouse'].astype(float)\n",
        "df['salesNearestWarehousesRatio'] = df['salesNearestWarehousesRatio'].astype(float)\n",
        "df['avgDistanceNearest5Supermarkets'] = df['avgDistanceNearest5Supermarkets'].astype(float)\n",
        "df['salesNearest5StoresRatio'] = df['salesNearest5StoresRatio'].astype(float)\n",
        "\n",
        "\n",
        "# Time Series\n",
        "data_subset = df[df.storeId.isin(use_stores)]\n",
        "nseries = data_subset.groupby(grain_column_names).ngroups\n",
        "print('Data subset contains {0} individual time-series.'.format(nseries))\n",
        "\n",
        "# Group by date\n",
        "def split_last_n_by_grain(df, n):\n",
        "    \"\"\"Group df by grain and split on last n rows for each group.\"\"\"\n",
        "    df_grouped = (df.sort_values(time_column_name) # Sort by ascending time\n",
        "                  .groupby(grain_column_names, group_keys=False))\n",
        "    df_head = df_grouped.apply(lambda dfg: dfg.iloc[:-n])\n",
        "    df_tail = df_grouped.apply(lambda dfg: dfg.iloc[-n:])\n",
        "    return df_head, df_tail\n",
        "\n",
        "# splitting\n",
        "train, test = split_last_n_by_grain(data_subset, n_test_periods)\n",
        "print(len(train),len(test))\n",
        "train.to_csv (r'./SurfaceSales_train.csv', index = None, header=True)\n",
        "test.to_csv (r'./SurfaceSales_test.csv', index = None, header=True)\n",
        "datastore = ws.get_default_datastore()\n",
        "datastore.upload_files(files = ['./SurfaceSales_train.csv', './SurfaceSales_test.csv'], target_path = 'dataset/', overwrite = True,show_progress = True)\n",
        "\n",
        "# loading the train dataset\n",
        "from azureml.core.dataset import Dataset\n",
        "train_dataset = Dataset.Tabular.from_delimited_files(path=datastore.path('dataset/SurfaceSales_train.csv'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Training the Models using AutoML Forecasting\n",
        "\n",
        "Please notice that **compute_target** is commented, meaning that the model training will run locally in Synapse Spark.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "time_series_settings = {\n",
        "    'time_column_name': time_column_name,\n",
        "    'grain_column_names': grain_column_names,\n",
        "    'max_horizon': n_test_periods\n",
        "}\n",
        "\n",
        "# Config\n",
        "automl_config = AutoMLConfig(task='forecasting',\n",
        "                             debug_log='automl_ss_sales_errors.log',\n",
        "                             primary_metric='normalized_mean_absolute_error',\n",
        "                             experiment_timeout_hours=0.25,\n",
        "                             training_data=train_dataset,\n",
        "                             label_column_name=target_column_name,\n",
        "                             #compute_target=compute_target,\n",
        "                             enable_early_stopping=True,\n",
        "                             n_cross_validations=3,\n",
        "                             verbosity=logging.INFO,\n",
        "                             **time_series_settings)\n",
        "\n",
        "# Running the training\n",
        "remote_run = experiment.submit(automl_config, show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Retrieving the Best Model and Forecasting\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieving the best model\n",
        "best_run, fitted_model = remote_run.get_output()\n",
        "print(fitted_model.steps)\n",
        "model_name = best_run.properties['model_name']\n",
        "print(model_name)\n",
        "\n",
        "# Forecasting based on test dataset\n",
        "X_test = test\n",
        "y_test = X_test.pop(target_column_name).values\n",
        "X_test[time_column_name] = pd.to_datetime(X_test[time_column_name])\n",
        "y_predictions, X_trans = fitted_model.forecast(X_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Plotting the Results\n",
        "\n",
        "At this point you should have a chart that created with AutoML and MatplotLib. \n",
        "\n",
        "The results are that good because of the **logQuantity** column, a  data Leakage calculated from **quantity** column. You can try to run the same experiment without it.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pandas.tseries.frequencies import to_offset\n",
        "\n",
        "\n",
        "def align_outputs(y_predicted, X_trans, X_test, y_test, target_column_name,\n",
        "                  predicted_column_name='predicted',\n",
        "                  horizon_colname='horizon_origin'):\n",
        "    \"\"\"\n",
        "    Demonstrates how to get the output aligned to the inputs\n",
        "    using pandas indexes. Helps understand what happened if\n",
        "    the output's shape differs from the input shape, or if\n",
        "    the data got re-sorted by time and grain during forecasting.\n",
        "\n",
        "    Typical causes of misalignment are:\n",
        "    * we predicted some periods that were missing in actuals -> drop from eval\n",
        "    * model was asked to predict past max_horizon -> increase max horizon\n",
        "    * data at start of X_test was needed for lags -> provide previous periods\n",
        "    \"\"\"\n",
        "\n",
        "    if (horizon_colname in X_trans):\n",
        "        df_fcst = pd.DataFrame({predicted_column_name: y_predicted,\n",
        "                                horizon_colname: X_trans[horizon_colname]})\n",
        "    else:\n",
        "        df_fcst = pd.DataFrame({predicted_column_name: y_predicted})\n",
        "\n",
        "    # y and X outputs are aligned by forecast() function contract\n",
        "    df_fcst.index = X_trans.index\n",
        "\n",
        "    # align original X_test to y_test\n",
        "    X_test_full = X_test.copy()\n",
        "    X_test_full[target_column_name] = y_test\n",
        "\n",
        "    # X_test_full's index does not include origin, so reset for merge\n",
        "    df_fcst.reset_index(inplace=True)\n",
        "    X_test_full = X_test_full.reset_index().drop(columns='index')\n",
        "    together = df_fcst.merge(X_test_full, how='right')\n",
        "\n",
        "    # drop rows where prediction or actuals are nan\n",
        "    # happens because of missing actuals\n",
        "    # or at edges of time due to lags/rolling windows\n",
        "    clean = together[together[[target_column_name,\n",
        "                               predicted_column_name]].notnull().all(axis=1)]\n",
        "    return(clean)\n",
        "\n",
        "\n",
        "df_all = align_outputs(y_predictions, X_trans, X_test, y_test, target_column_name)\n",
        "\n",
        "#from azureml.automl.core._vendor.automl.client.core.common import metrics\n",
        "from matplotlib import pyplot as plt\n",
        "from automl.client.core.common import constants\n",
        "\n",
        "# use automl metrics module\n",
        "#scores = metrics.compute_metrics_regression(\n",
        "#    df_all['predicted'],\n",
        "#    df_all[target_column_name],\n",
        "#    list(constants.Metric.SCALAR_REGRESSION_SET),\n",
        "#    None, None, None)\n",
        "\n",
        "#print(\"[Test data scores]\\n\")\n",
        "#for key, value in scores.items():    \n",
        "#    print('{}:   {:.3f}'.format(key, value))\n",
        "    \n",
        "# Plot outputs\n",
        "#%matplotlib inline\n",
        "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='b')\n",
        "test_test = plt.scatter(df_all[target_column_name], df_all[target_column_name], color='g')\n",
        "plt.legend((test_pred, test_test), ('prediction', 'truth'), loc='upper left', fontsize=8)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Register Model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = best_run.properties['model_name']\n",
        "\n",
        "script_file_name = 'inference/score.py'\n",
        "conda_env_file_name = 'inference/env.yml'\n",
        "\n",
        "best_run.download_file('outputs/scoring_file_v_1_0_0.py', 'inference/score.py')\n",
        "best_run.download_file('outputs/conda_env_v_1_0_0.yml', 'inference/env.yml')\n",
        "\n",
        "registered_model = remote_run.register_model(model_name = model_name)\n",
        "\n",
        "print(remote_run.model_id)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Deploy Web Service as Azure Container Instance"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.webservice import AciWebservice, Webservice\n",
        "\n",
        "aci_config = AciWebservice.deploy_configuration(\n",
        "   cpu_cores = 1, \n",
        "   memory_gb = 2, \n",
        "   tags = {'name':'scoring'}, \n",
        "   description = 'Scoring web service')\n",
        "\n",
        "from azureml.core.model import InferenceConfig\n",
        "inference_config = InferenceConfig(runtime=\"python\", \n",
        "                                       entry_script=script_file_name,\n",
        "                                       conda_file=conda_env_file_name)\n",
        "\n",
        "from azureml.core.model import Model\n",
        "webservice = Model.deploy(workspace=ws,\n",
        "                              name=\"scoringservice\",\n",
        "                              models=[registered_model],\n",
        "                              inference_config=inference_config,\n",
        "                              deployment_config=aci_config)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 20. Cleansing Resources"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "drop table CosmosDemoIoT.IoTSignals"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "drop table CosmosDemoIoT.IoTDeviceInfo"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "drop table CosmosDemoRetail.RetailProducts"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "drop table CosmosDemoRetail.RetailSales"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "drop table CosmosDemoRetail.RetailStoreDemographics"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "drop database CosmosDemoIoT"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\r\n",
        "\r\n",
        "drop database CosmosDemoRetail"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}