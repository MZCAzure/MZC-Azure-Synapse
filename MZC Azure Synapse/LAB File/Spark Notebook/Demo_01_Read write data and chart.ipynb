{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": false,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01. Load a sample data \n",
        "\n",
        "Let's first load the [public holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from azureml.opendatasets import PublicHolidays\n",
        "\n",
        "from datetime import datetime\n",
        "from dateutil import parser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "end_date = datetime.today()\n",
        "start_date = datetime.today() - relativedelta(months=6)\n",
        "hol = PublicHolidays(start_date=start_date, end_date=end_date)\n",
        "hol_df = hol.to_spark_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Display 5 rows\n",
        "hol_df.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Write data to the default ADLS Gen2 storage\n",
        "\n",
        "We are going to write the spark dateframe to your default ADLS Gen2 storage account.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Primary storage info\n",
        "account_name = 'adlsgen2adlsgen2' # fill in your primary account name\n",
        "container_name = 'sparkdemo' # fill in your container name\n",
        "relative_path = 'sparkdemo01/' # fill in your relative folder path\n",
        "\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\n",
        "print('Primary storage account path: ' + adls_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 02-1. Save a dataframe as Parquet, JSON, or CSV\n",
        "If you have a dataframe, you can save it to Parquet or JSON with the .write.parquet(), .write.json() and .write.csv() methods respectively.\n",
        "\n",
        "Dataframes can be saved in any format, regardless of the input format.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "parquet_path = adls_path + 'holiday.parquet'\n",
        "json_path = adls_path + 'holiday.json'\n",
        "csv_path = adls_path + 'holiday.csv'\n",
        "print('parquet file path: ' + parquet_path)\n",
        "print('json file pathï¼š ' + json_path)\n",
        "print('csv file path: ' + csv_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "hol_df.write.parquet(parquet_path, mode = 'overwrite')\n",
        "hol_df.write.json(json_path, mode = 'overwrite')\n",
        "hol_df.write.csv(csv_path, mode = 'overwrite', header = 'true')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 02-2. Read data from the default ADLS Gen2 storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "df_parquet = spark.read.parquet(parquet_path)\r\n",
        "df_parquet.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "df_json = spark.read.json(json_path)\r\n",
        "df_json.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "df_csv = spark.read.csv(csv_path, header = 'true')\r\n",
        "df_csv.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# 03. Write data to the Delta Lake table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Delta Lake relative path\r\n",
        "delta_relative_path = adls_path + 'delta/holiday/'\r\n",
        "print('Delta Lake path: ' + delta_relative_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Filter out indian holidays\r\n",
        "hol_df_IN = hol_df[(hol_df.countryRegionCode == \"IN\")]\r\n",
        "hol_df_IN.show(5, truncate = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "#Let's write the data in the Delta Lake table. \r\n",
        "hol_df_IN.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"holidayName\").save(delta_relative_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "delta_data = spark.read.format(\"delta\").load(delta_relative_path)\r\n",
        "delta_data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Overwrite the entire Delta Lake table\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "#Let's overwrite the entire delta file with 1 record\r\n",
        "\r\n",
        "hol_df_JP= hol_df[(hol_df.countryRegionCode == \"JP\")]\r\n",
        "hol_df_JP.write.format(\"delta\").mode(\"overwrite\").save(delta_relative_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "delta_data = spark.read.format(\"delta\").load(delta_relative_path)\r\n",
        "delta_data.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Merge new data based on given merge condition "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Upsert (merge) the United States' holiday data with Japan's\r\n",
        " \r\n",
        "from delta.tables import *\r\n",
        "\r\n",
        "deltaTable = DeltaTable.forPath(spark,delta_relative_path)\r\n",
        "\r\n",
        "hol_df_US= hol_df[(hol_df.countryRegionCode == \"US\")]\r\n",
        "\r\n",
        "\r\n",
        "deltaTable.alias(\"hol_df_JP\").merge(\r\n",
        "     source = hol_df_US.alias(\"hol_df_US\"),\r\n",
        "     condition = \"hol_df_JP.countryRegionCode = hol_df_US.countryRegionCode\"\r\n",
        "    ).whenMatchedUpdate(set = \r\n",
        "    {}).whenNotMatchedInsert( values = \r\n",
        "    {\r\n",
        "        \"countryOrRegion\" : \"hol_df_US.countryOrRegion\",\r\n",
        "        \"holidayName\" : \"hol_df_US.holidayName\",\r\n",
        "        \"normalizeHolidayName\" : \"hol_df_US.normalizeHolidayName\",\r\n",
        "        \"isPaidTimeOff\":\"hol_df_US.isPaidTimeOff\",\r\n",
        "        \"countryRegionCode\":\"hol_df_US.countryRegionCode\",\r\n",
        "        \"date\":\"hol_df_US.date\"\r\n",
        "    }\r\n",
        "    ).execute()\r\n",
        "\r\n",
        "\r\n",
        "deltaTable.toDF().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Update table on the rows that match the given condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Update column the 'null' value in 'isPaidTimeOff' with 'false'\r\n",
        "\r\n",
        "from pyspark.sql.functions import *\r\n",
        "deltaTable.update(\r\n",
        "    condition = (col(\"isPaidTimeOff\").isNull()),\r\n",
        "    set = {\"isPaidTimeOff\": \"false\"})\r\n",
        "\r\n",
        "deltaTable.toDF().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Delete data from the table that match the given condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "print(\"Row count before delete: \")\r\n",
        "print(deltaTable.toDF().count())\r\n",
        "\r\n",
        "\r\n",
        "# Delte data with date later than 2020-01-01\r\n",
        "deltaTable.delete (\"date > '2020-01-01'\")\r\n",
        "\r\n",
        "\r\n",
        "print(\"Row count after delete:  \")\r\n",
        "print(deltaTable.toDF().count())\r\n",
        "deltaTable.toDF().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Get the operation history of the delta table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "fullHistoryDF = deltaTable.history()\r\n",
        "lastOperationDF = deltaTable.history(1)\r\n",
        "\r\n",
        "print('Full history DF: ')\r\n",
        "fullHistoryDF.show(truncate = False)\r\n",
        "\r\n",
        "print('lastOperationDF: ')\r\n",
        "lastOperationDF.show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load a sample data\r\n",
        "\r\n",
        "Let's first load the [Public Holidays](https://azure.microsoft.com/en-us/services/open-datasets/catalog/public-holidays/) of last 6 months from Azure Open datasets as a sample.\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": true
      },
      "source": [
        "%%pyspark \r\n",
        "# Load sample data from azure open dataset in pyspark\r\n",
        "from azureml.opendatasets import PublicHolidays\r\n",
        "\r\n",
        "from datetime import datetime\r\n",
        "from dateutil import parser\r\n",
        "from dateutil.relativedelta import relativedelta\r\n",
        "\r\n",
        "\r\n",
        "end_date = datetime.today()\r\n",
        "start_date = datetime.today() - relativedelta(months=6)\r\n",
        "hol = PublicHolidays(start_date=start_date, end_date=end_date)\r\n",
        "hol_df = hol.to_spark_dataframe()\r\n",
        "\r\n",
        "print('Register the DataFrame as a SQL temporary view: source')\r\n",
        "hol_df.createOrReplaceTempView('source')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": true
      },
      "source": [
        "%%spark\r\n",
        "// Remove datetime from the data source\r\n",
        "val holiday_nodate = spark.sql(\"SELECT countryOrRegion, holidayName, normalizeHolidayName,isPaidTimeOff,countryRegionCode FROM source\")\r\n",
        "holiday_nodate.show(5,truncate = false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Write a Spark dataframe into your sql pool\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": true
      },
      "source": [
        "%%spark\r\n",
        "// Write the dataframe into your sql pool\r\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
        "import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
        "\r\n",
        "val sql_pool_name = \"sqlpool\" //fill in your sql pool name\r\n",
        "\r\n",
        "holiday_nodate.write.sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\", Constants.INTERNAL)\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read from a SQL Pool table with Spark\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": true
      },
      "source": [
        "%%spark\r\n",
        "// Read  the table we just created in the sql pool as a Spark dataframe\r\n",
        "val spark_read = spark.read.\r\n",
        "    sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\")\r\n",
        "spark_read.show(5, truncate = false)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Charting in Synapse Notebook\r\n",
        "\r\n",
        "Synapse has common used data visualization packages pre installed, such as **matplotlib**, **bokeh**, **seaborn**, **altair**, **plotly**. This notebook provides examples to do data visualization using charts in Synapse notebook. \r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Matplotlib\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Line charts\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        " \r\n",
        "x  = [1, 2, 3, 4, 5, 6, 7, 8, 9]\r\n",
        "y1 = [1, 3, 5, 3, 1, 3, 5, 3, 1]\r\n",
        "y2 = [2, 4, 6, 4, 2, 4, 6, 4, 2]\r\n",
        "plt.plot(x, y1, label=\"line L\")\r\n",
        "plt.plot(x, y2, label=\"line H\")\r\n",
        "plt.plot()\r\n",
        "\r\n",
        "plt.xlabel(\"x axis\")\r\n",
        "plt.ylabel(\"y axis\")\r\n",
        "plt.title(\"Line Graph Example\")\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Bar chart\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "# Look at index 4 and 6, which demonstrate overlapping cases.\r\n",
        "x1 = [1, 3, 4, 5, 6, 7, 9]\r\n",
        "y1 = [4, 7, 2, 4, 7, 8, 3]\r\n",
        "\r\n",
        "x2 = [2, 4, 6, 8, 10]\r\n",
        "y2 = [5, 6, 2, 6, 2]\r\n",
        "\r\n",
        "# Colors: https://matplotlib.org/api/colors_api.html\r\n",
        "\r\n",
        "plt.bar(x1, y1, label=\"Blue Bar\", color='b')\r\n",
        "plt.bar(x2, y2, label=\"Green Bar\", color='g')\r\n",
        "plt.plot()\r\n",
        "\r\n",
        "plt.xlabel(\"bar number\")\r\n",
        "plt.ylabel(\"bar height\")\r\n",
        "plt.title(\"Bar Chart Example\")\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Histogram\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Use numpy to generate a bunch of random data in a bell curve around 5.\r\n",
        "n = 5 + np.random.randn(1000)\r\n",
        "\r\n",
        "m = [m for m in range(len(n))]\r\n",
        "plt.bar(m, n)\r\n",
        "plt.title(\"Raw Data\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.hist(n, bins=20)\r\n",
        "plt.title(\"Histogram\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.hist(n, cumulative=True, bins=20)\r\n",
        "plt.title(\"Cumulative Histogram\")\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Scatter chart\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "x1 = [2, 3, 4]\r\n",
        "y1 = [5, 5, 5]\r\n",
        "\r\n",
        "x2 = [1, 2, 3, 4, 5]\r\n",
        "y2 = [2, 3, 2, 3, 4]\r\n",
        "y3 = [6, 8, 7, 8, 7]\r\n",
        "\r\n",
        "# Markers: https://matplotlib.org/api/markers_api.html\r\n",
        "\r\n",
        "plt.scatter(x1, y1)\r\n",
        "plt.scatter(x2, y2, marker='v', color='r')\r\n",
        "plt.scatter(x2, y3, marker='^', color='m')\r\n",
        "plt.title('Scatter Plot Example')\r\n",
        "plt.show()\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Stack plots\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "idxes = [ 1,  2,  3,  4,  5,  6,  7,  8,  9]\r\n",
        "arr1  = [23, 40, 28, 43,  8, 44, 43, 18, 17]\r\n",
        "arr2  = [17, 30, 22, 14, 17, 17, 29, 22, 30]\r\n",
        "arr3  = [15, 31, 18, 22, 18, 19, 13, 32, 39]\r\n",
        "\r\n",
        "# Adding legend for stack plots is tricky.\r\n",
        "plt.plot([], [], color='r', label = 'D 1')\r\n",
        "plt.plot([], [], color='g', label = 'D 2')\r\n",
        "plt.plot([], [], color='b', label = 'D 3')\r\n",
        "\r\n",
        "plt.stackplot(idxes, arr1, arr2, arr3, colors= ['r', 'g', 'b'])\r\n",
        "plt.title('Stack Plot Example')\r\n",
        "plt.legend()\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Pie charts\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "labels = 'S1', 'S2', 'S3'\r\n",
        "sections = [56, 66, 24]\r\n",
        "colors = ['c', 'g', 'y']\r\n",
        "\r\n",
        "plt.pie(sections, labels=labels, colors=colors,\r\n",
        "        startangle=90,\r\n",
        "        explode = (0, 0.1, 0),\r\n",
        "        autopct = '%1.2f%%')\r\n",
        "\r\n",
        "plt.axis('equal') # Try commenting this out.\r\n",
        "plt.title('Pie Chart Example')\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# fill_between and alpha\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "ys = 200 + np.random.randn(100)\r\n",
        "x = [x for x in range(len(ys))]\r\n",
        "\r\n",
        "plt.plot(x, ys, '-')\r\n",
        "plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\r\n",
        "\r\n",
        "plt.title(\"Fills and Alpha Example\")\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Subplotting using Subplot2grid\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def random_plots():\r\n",
        "  xs = []\r\n",
        "  ys = []\r\n",
        "  \r\n",
        "  for i in range(20):\r\n",
        "    x = i\r\n",
        "    y = np.random.randint(10)\r\n",
        "    \r\n",
        "    xs.append(x)\r\n",
        "    ys.append(y)\r\n",
        "  \r\n",
        "  return xs, ys\r\n",
        "\r\n",
        "fig = plt.figure()\r\n",
        "ax1 = plt.subplot2grid((5, 2), (0, 0), rowspan=1, colspan=2)\r\n",
        "ax2 = plt.subplot2grid((5, 2), (1, 0), rowspan=3, colspan=2)\r\n",
        "ax3 = plt.subplot2grid((5, 2), (4, 0), rowspan=1, colspan=1)\r\n",
        "ax4 = plt.subplot2grid((5, 2), (4, 1), rowspan=1, colspan=1)\r\n",
        "\r\n",
        "x, y = random_plots()\r\n",
        "ax1.plot(x, y)\r\n",
        "\r\n",
        "x, y = random_plots()\r\n",
        "ax2.plot(x, y)\r\n",
        "\r\n",
        "x, y = random_plots()\r\n",
        "ax3.plot(x, y)\r\n",
        "\r\n",
        "x, y = random_plots()\r\n",
        "ax4.plot(x, y)\r\n",
        "\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# 3D Scatter Plots\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "from mpl_toolkits.mplot3d import axes3d\r\n",
        "\r\n",
        "fig = plt.figure()\r\n",
        "ax = fig.add_subplot(111, projection = '3d')\r\n",
        "\r\n",
        "x1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n",
        "y1 = np.random.randint(10, size=10)\r\n",
        "z1 = np.random.randint(10, size=10)\r\n",
        "\r\n",
        "x2 = [-1, -2, -3, -4, -5, -6, -7, -8, -9, -10]\r\n",
        "y2 = np.random.randint(-10, 0, size=10)\r\n",
        "z2 = np.random.randint(10, size=10)\r\n",
        "\r\n",
        "ax.scatter(x1, y1, z1, c='b', marker='o', label='blue')\r\n",
        "ax.scatter(x2, y2, z2, c='g', marker='D', label='green')\r\n",
        "\r\n",
        "ax.set_xlabel('x axis')\r\n",
        "ax.set_ylabel('y axis')\r\n",
        "ax.set_zlabel('z axis')\r\n",
        "plt.title(\"3D Scatter Plot Example\")\r\n",
        "plt.legend()\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# 3D Bar Plots\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "fig = plt.figure()\r\n",
        "ax = fig.add_subplot(111, projection = '3d')\r\n",
        "\r\n",
        "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n",
        "y = np.random.randint(10, size=10)\r\n",
        "z = np.zeros(10)\r\n",
        "\r\n",
        "dx = np.ones(10)\r\n",
        "dy = np.ones(10)\r\n",
        "dz = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\r\n",
        "\r\n",
        "ax.bar3d(x, y, z, dx, dy, dz, color='g')\r\n",
        "\r\n",
        "ax.set_xlabel('x axis')\r\n",
        "ax.set_ylabel('y axis')\r\n",
        "ax.set_zlabel('z axis')\r\n",
        "plt.title(\"3D Bar Chart Example\")\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Wireframe Plots\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "fig = plt.figure()\r\n",
        "ax = fig.add_subplot(111, projection = '3d')\r\n",
        "\r\n",
        "x, y, z = axes3d.get_test_data()\r\n",
        "\r\n",
        "ax.plot_wireframe(x, y, z, rstride = 2, cstride = 2)\r\n",
        "\r\n",
        "plt.title(\"Wireframe Plot Example\")\r\n",
        "plt.tight_layout()\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Seaborn\r\n",
        "Seaborn is a library layered on top of Matplotlib that you can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Scatterplot with a nice regression line fit to it, all with just one call to Seaborn's regplot.\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "# Generate some random data\r\n",
        "num_points = 20\r\n",
        "# x will be 5, 6, 7... but also twiddled randomly\r\n",
        "x = 5 + np.arange(num_points) + np.random.randn(num_points)\r\n",
        "# y will be 10, 11, 12... but twiddled even more randomly\r\n",
        "y = 10 + np.arange(num_points) + 5 * np.random.randn(num_points)\r\n",
        "sns.regplot(x, y)\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Seanborn heatmap\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Make a 10 x 10 heatmap of some random data\r\n",
        "side_length = 10\r\n",
        "# Start with a 10 x 10 matrix with values randomized around 5\r\n",
        "data = 5 + np.random.randn(side_length, side_length)\r\n",
        "# The next two lines make the values larger as we get closer to (9, 9)\r\n",
        "data += np.arange(side_length)\r\n",
        "data += np.reshape(np.arange(side_length), (side_length, 1))\r\n",
        "# Generate the heatmap\r\n",
        "sns.heatmap(data)\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Bokeh\r\n",
        "You can render HTML or interactive libraries, like **bokeh**, using the **displayHTML()**.\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "import numpy as np\r\n",
        "from bokeh.plotting import figure, show\r\n",
        "from bokeh.io import output_notebook\r\n",
        "from bokeh.embed import file_html\r\n",
        "from bokeh.resources import CDN\r\n",
        "\r\n",
        "N = 4000\r\n",
        "x = np.random.random(size=N) * 100\r\n",
        "y = np.random.random(size=N) * 100\r\n",
        "radii = np.random.random(size=N) * 1.5\r\n",
        "colors = [\"#%02x%02x%02x\" % (r, g, 150) for r, g in zip(np.floor(50+2*x).astype(int), np.floor(30+2*y).astype(int))]\r\n",
        "\r\n",
        "p = figure()\r\n",
        "p.circle(x, y, radius=radii, fill_color=colors, fill_alpha=0.6, line_color=None)\r\n",
        "show(p)\r\n",
        "\r\n",
        "# create an html document that embeds the Bokeh plot\r\n",
        "html = file_html(p, CDN, \"my plot1\")\r\n",
        "\r\n",
        "# display this html\r\n",
        "displayHTML(html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Plotting glyphs over a map using bokeh.\r\n",
        "\r\n",
        "from bokeh.plotting import figure, output_file\r\n",
        "from bokeh.tile_providers import get_provider, Vendors\r\n",
        "from bokeh.embed import file_html\r\n",
        "from bokeh.resources import CDN\r\n",
        "from bokeh.models import ColumnDataSource\r\n",
        "\r\n",
        "tile_provider = get_provider(Vendors.CARTODBPOSITRON)\r\n",
        "\r\n",
        "# range bounds supplied in web mercator coordinates\r\n",
        "p = figure(x_range=(-9000000,-8000000), y_range=(4000000,5000000),\r\n",
        "           x_axis_type=\"mercator\", y_axis_type=\"mercator\")\r\n",
        "p.add_tile(tile_provider)\r\n",
        "\r\n",
        "# plot datapoints on the map\r\n",
        "source = ColumnDataSource(\r\n",
        "    data=dict(x=[ -8800000, -8500000 , -8800000],\r\n",
        "              y=[4200000, 4500000, 4900000])\r\n",
        ")\r\n",
        "\r\n",
        "p.circle(x=\"x\", y=\"y\", size=15, fill_color=\"blue\", fill_alpha=0.8, source=source)\r\n",
        "\r\n",
        "# create an html document that embeds the Bokeh plot\r\n",
        "html = file_html(p, CDN, \"my plot1\")\r\n",
        "\r\n",
        "# display this html\r\n",
        "displayHTML(html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": []
    }
  ]
}